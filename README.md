## About study

<p align="justify">This web page presents a large-scale empirical study (based on 3,290 systems) to investigate whether and how thresholds vary among systems of different domains. We argue that the definition of appropriate thresholds needs to be tailored to each metric by the software domain. Therefore, we defend the idea of domain-specific thresholds, given that systems from different domains may have distinct characteristics and, it may impair the derived metric thresholds. As consequence, it can increase the number of false positive anomalies, for instance.</p> 


<p align="justify">This study relies on fifteen software domains composed of object-oriented Java systems. We apply to each system a set of eight well-known source code metrics. After applying the measurements, we derived 90% and 95% thresholds for each metric per domain, compared, and analyzed them in different ways. For instance, we compared the thresholds among domains and analyzed the effectiveness of code smell detection between domain-specific and generic thresholds; i.e., derived from heterogeneous systems from several domains. </p>

## Downloads

We available all data used in this study. 

To download PDF about software domains, click in <a href="https://github.com/saner2018/eds/blob/master/Oracle.pdf" download="Download"><img src="https://cdn2.iconfinder.com/data/icons/snipicons/5000/download-alt-48.png" /></a>


To download of the thresholds derivations in this study, click in [link](https://github.com/saner2018/eds/blob/master/Thresholds.xlsx). 


To download of the metrics of each  software domains used in this study, click in [link](https://github.com/saner2018/eds/blob/master/Metrics.rar)
